{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from keras.models import Sequential,Model\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Activation, Input,RepeatVector,Embedding, Flatten, Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn import metrics as mt\n",
    "from keras import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import average, concatenate,RepeatVector,Lambda,add,subtract\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import random\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import pickle\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,GlobalMaxPooling1D,MaxPooling1D,Reshape,Add\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D,AveragePooling1D\n",
    "from keras.regularizers import l2 \n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroe(y_true,y_pred):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"ACC:\",accuracy_score(y_true, y_pred))\n",
    "\n",
    "#Prepare Data\n",
    "import numpy as np\n",
    "# with open('final_data_dude_rd2.npy', 'rb') as handle:\n",
    "data = np.load('final_data.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Class 1: 8\n",
      "Class 0: 11\n",
      "['1FLT', '1D4H', '1IVO']\n",
      "(19, 9000, 2) (19, 512) (19,)\n"
     ]
    }
   ],
   "source": [
    "# take 80 % of proteins to train\n",
    "\n",
    "random.seed(23456)\n",
    "\n",
    "X, proteins, ic50 = data['X'], data['proteins'], data['score']\n",
    "print(len(X))\n",
    "k = sum(ic50)\n",
    "\n",
    "print ('Class 1:',k)\n",
    "print(\"Class 0:\",(len(ic50)-sum(ic50)) )\n",
    "\n",
    "td = {}\n",
    "\n",
    "pr = data['pr_names']\n",
    "\n",
    "for i in pr:\n",
    "\tif i not in td:\n",
    "\t\ttd[i] = 1\n",
    "\telse:\n",
    "\t\ttd[i] += 1\n",
    "li = [[td[i], i] for i in td]\n",
    "\n",
    "li = [i[1] for i in li]\n",
    "\n",
    "random.shuffle(li)#.shuffle()\n",
    "\n",
    "print (li)\n",
    "\n",
    "def get_fac():\n",
    "    \n",
    "    train = li[:int(0.8*len(li))]\n",
    "    \n",
    "    fac = []\n",
    "    fac_t = []\n",
    "\n",
    "    pro_dic = {}\n",
    "\n",
    "    for i in range (len(pr)):\n",
    "        if pr[i] in train:\n",
    "            fac.append(i)\n",
    "        else:\n",
    "            if pr[i] in pro_dic:\n",
    "                pro_dic[pr[i]].append(i)\n",
    "            else:\n",
    "                pro_dic[pr[i]] = [i]\n",
    "            fac_t.append(i)\n",
    "    fac = np.array(fac)\n",
    "    fac_t = np.array(fac_t)\n",
    "    return fac, fac_t, pro_dic\n",
    "\n",
    "fac, fac_t, pro_dic = get_fac()\n",
    "\n",
    "proteins = np.array(proteins)\n",
    "X = np.array(X)\n",
    "ic50 = np.array(ic50)\n",
    "\n",
    "print (proteins.shape, X.shape, ic50.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 2)\n",
      "512\n",
      "(7, 512) (12, 512)\n"
     ]
    }
   ],
   "source": [
    "Proeins_shape=proteins.shape[1:]\n",
    "Drags_shape=X.shape[1]\n",
    "print(Proeins_shape)\n",
    "print(Drags_shape)\n",
    "\n",
    "def get_features():\n",
    "\n",
    "    train_X = X[fac]\n",
    "    valid_X = X[fac_t]\n",
    "\n",
    "    train_proteins = proteins[fac]\n",
    "    valid_proteins = proteins[fac_t]\n",
    "\n",
    "    train_y = ic50[fac]\n",
    "    valid_y = ic50[fac_t]\n",
    "    \n",
    "    return train_X, valid_X, train_proteins, valid_proteins, train_y, valid_y\n",
    "\n",
    "# In[11]:\n",
    "train_X, valid_X, train_proteins, valid_proteins, train_y, valid_y = get_features()\n",
    "\n",
    "print (train_X.shape, valid_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 512) (7,) (7, 9000, 2)\n",
      "(14, 512) (14,) (14, 9000, 2)\n"
     ]
    }
   ],
   "source": [
    "# add data by shuffling chains\n",
    "\n",
    "print (train_X.shape, train_y.shape, train_proteins.shape)\n",
    "\n",
    "\n",
    "def expand_train(train_X, train_y, train_proteins):\n",
    "    lis_X, lis_y, pro = [], [], []\n",
    "    for i in range (len(train_X)):\n",
    "\n",
    "        lis_X.append(train_X[i])\n",
    "        lis_y.append(train_y[i])\n",
    "        k = train_proteins[i].reshape((6, 1500, 2))\n",
    "        ik = random.shuffle(list(range(6)))\n",
    "        k = k[ik].reshape((9000, 2))\n",
    "        #a, b = k\n",
    "        #a = a[ik]\n",
    "        #b = b[ik]\n",
    "\n",
    "        #c = np.concatenate((a,b)).reshape(train_proteins.shape[1:])\n",
    "\n",
    "        #print (c.shape)\n",
    "        pro.append(k)\n",
    "        #if i%10000 ==0 and i !=0:\n",
    "        #    break\n",
    "            \n",
    "    train_X = np.concatenate((train_X, np.array(lis_X)))\n",
    "    train_y = np.concatenate((train_y, np.array(lis_y)))\n",
    "    train_proteins = np.concatenate((train_proteins, np.array(pro)))\n",
    "    \n",
    "    return train_X, train_y, train_proteins\n",
    "\n",
    "train_X, train_y, train_proteins = expand_train(train_X, train_y, train_proteins)\n",
    "    \n",
    "print (train_X.shape, train_y.shape, train_proteins.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callback_list_by_model(model_name):\n",
    " \n",
    "    checkpoint = ModelCheckpoint('model_zoo/'+model_name+'.h5', monitor='val_loss', verbose=1, \n",
    "                                 save_best_only=True, mode='min', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                       verbose=1, mode='auto', epsilon=0.0001)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", \n",
    "                          mode=\"min\", \n",
    "                          patience=7)\n",
    "    csv_logger = CSVLogger(filename='./training_log_'+model_name+'.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "    \n",
    "    callbacks_list = [checkpoint,reduceLROnPlat, early,csv_logger]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,model_name,Epoch):\n",
    "    callbacks_list = get_callback_list_by_model(model_name)\n",
    "#     train_proteins_shuffle, train_X_shuffle, train_y_shuffle = shuffle(train_proteins, train_X, train_y,random_state=randomS)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x=[train_proteins,train_X], # create a list of inputs for embeddings\n",
    "            y=train_y, epochs=Epoch, \n",
    "            batch_size=64, verbose=1,\n",
    "            validation_data = ([valid_proteins,valid_X],valid_y),\n",
    "            callbacks=callbacks_list         \n",
    "               )\n",
    "\n",
    "    re=np.round(np.squeeze(model.predict([valid_proteins,valid_X])))\n",
    "    scroe(re,list(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_blocks(ft_number,k_size,input_tensor):\n",
    "    x = Conv1D(filters=ft_number, \n",
    "                     kernel_size=k_size, data_format='channels_last',\n",
    "                     padding='same',\n",
    "                     kernel_regularizer=l2(l2_lambda)\n",
    "              )(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    \n",
    "#     x = Conv1D(filters=ft_number, \n",
    "#                      kernel_size=k_size, \n",
    "#                      padding='same',\n",
    "#                      kernel_regularizer=l2(l2_lambda)\n",
    "#               )(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    return x\n",
    "def op(inputs):\n",
    "    x, y = inputs\n",
    "    return K.pow((x - y), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_branch(init_input,kernel_size):\n",
    "    x=conv_blocks(ft_number=32,k_size=kernel_size,input_tensor=init_input)\n",
    "    #x=MaxPooling1D(2,padding='same')(x)\n",
    "\n",
    "    x=conv_blocks(ft_number=64,k_size=kernel_size,input_tensor=x)\n",
    "    #x=MaxPooling1D(2,padding='same')(x)\n",
    "\n",
    "    x=conv_blocks(ft_number=128,k_size=kernel_size,input_tensor=x)\n",
    "    u = GlobalMaxPooling1D()(x)\n",
    "    u_broadcast=RepeatVector(x.shape[1])(u)\n",
    "\n",
    "    o=Lambda(op)([u_broadcast,x])  # K.pow((x - y), 2) \n",
    "    var = GlobalMaxPooling1D()(o)\n",
    "    X_vector = concatenate([u,var])\n",
    "    \n",
    "    #X_vector = Dense(64)(X_vector)\n",
    "    return X_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda=0.05\n",
    "def create_multiBranch_Conv_model(Proeins_shape,Drags_shape):\n",
    "    \n",
    "    # left branch --> dimension reduction for Proeins_shape\n",
    "    \n",
    "    proeins_input_tensor = Input(shape=Proeins_shape, name='proeins_input_tensor')\n",
    "    #init_input = Reshape((Proeins_shape[1], Proeins_shape[0]),input_shape=Proeins_shape,name='init_input')(proeins_input_tensor)\n",
    "    init_input = proeins_input_tensor\n",
    "    #branch 0\n",
    "    w=conv_branch(init_input,10)\n",
    "    #branch 1\n",
    "    x=conv_branch(init_input,5)\n",
    "\n",
    "    #branch 2\n",
    "    y=conv_branch(init_input,15)\n",
    "    \n",
    "    #branch 3\n",
    "    z=conv_branch(init_input,20)\n",
    "    \n",
    "    protein_concat = concatenate([w,x,y,z], name='protein_concat_')\n",
    "    \n",
    "    protein_dense = Dense(128)(protein_concat)\n",
    "    protein_dense = BatchNormalization()(protein_dense)\n",
    "    protein_dense = Activation('relu')(protein_dense)\n",
    "    protein_dense = Dropout(0.5)(protein_dense)\n",
    "    \n",
    "    \n",
    "    # right branch --> dimension reduction for drug/ligand\n",
    "    drag_input_tensor = Input(shape=(Drags_shape,),name='drag_input_tensor')\n",
    "    d = Dense(128)(drag_input_tensor)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Activation('relu')(d)\n",
    "    d = Dropout(0.5)(d)\n",
    "\n",
    "    \n",
    "#     # merge the branches together\n",
    "    final_branch = concatenate([protein_dense,d], name='protein_darg_concat_')\n",
    "    \n",
    "    final_dense = Dense(64)(final_branch)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Activation('relu')(final_dense)\n",
    "    final_dense = Dropout(0.5)(final_dense)\n",
    "\n",
    "#     final_dense = Dense(32)(final_dense)\n",
    "#     final_dense = BatchNormalization()(final_dense)\n",
    "#     final_dense = Activation('relu')(final_dense)\n",
    "#     final_dense = Dropout(0.5)(final_dense)\n",
    "    \n",
    "    final_output = Dense(1, activation='sigmoid', name='final_output')(final_dense)\n",
    "    \n",
    "#     model = Model(inputs=proeins_input_tensor, outputs=protein_concat)\n",
    "    model = Model(inputs=[proeins_input_tensor,drag_input_tensor], outputs=final_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.6/site-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 14s 1s/step - loss: 26.5878 - acc: 0.5714 - val_loss: 25.7387 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 25.73872, saving model to model_zoo/multiBranch_Conv_model_copy.h5\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 25.7722 - acc: 0.5000 - val_loss: 24.6818 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_loss improved from 25.73872 to 24.68184, saving model to model_zoo/multiBranch_Conv_model_copy.h5\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 24.6732 - acc: 0.6429 - val_loss: 23.5747 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00003: val_loss improved from 24.68184 to 23.57465, saving model to model_zoo/multiBranch_Conv_model_copy.h5\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 23.5787 - acc: 0.6429 - val_loss: 22.5436 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00004: val_loss improved from 23.57465 to 22.54364, saving model to model_zoo/multiBranch_Conv_model_copy.h5\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 22.4732 - acc: 0.8571 - val_loss: 21.4578 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00005: val_loss improved from 22.54364 to 21.45782, saving model to model_zoo/multiBranch_Conv_model_copy.h5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.75      0.86        12\n",
      "        1.0       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.75      0.86        12\n",
      "\n",
      "[[9 3]\n",
      " [0 0]]\n",
      "ACC: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if 'model_zoo' not in os.listdir('.'):\n",
    "    os.mkdir('model_zoo')\n",
    "    \n",
    "multiBranch_Conv_model = create_multiBranch_Conv_model(Proeins_shape,Drags_shape,)\n",
    "#multiBranch_Conv_model.summary()\n",
    "evaluate_model(multiBranch_Conv_model,'multiBranch_Conv_model_copy',5)\n",
    "# SVG(model_to_dot(multiBranch_Conv_model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "multiBranch_Conv_model = create_multiBranch_Conv_model(Proeins_shape,Drags_shape,)\n",
    "multiBranch_Conv_model.load_weights('model_zoo/multiBranch_Conv_model_copy.h5')\n",
    "re1 =np.squeeze(multiBranch_Conv_model.predict([valid_proteins,valid_X]))\n",
    "print ('AUC', roc_auc_score(valid_y, re1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
